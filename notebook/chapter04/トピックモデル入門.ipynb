{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "from __future__ import print_function\n",
    "\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import codecs\n",
    "\n",
    "import lda\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>データを読み込もう</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.normpath('./dataset/document_word_data.json'), 'r') as f:\n",
    "    doc_data = json.load(f)\n",
    "\n",
    "all_doc_index = doc_data.keys()\n",
    "print('Total Documents: ', len(all_doc_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Windowsの方でutf-8のテキストファイルが開けない場合は、こちらを実行してみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Windowsの方で上記のコードが失敗する場合\n",
    "with codecs.open(os.path.normpath('./dataset/document_word_data.json'), 'r', \"utf-8\") as f:\n",
    "    doc_data = json.load(f)\n",
    "\n",
    "all_doc_index = doc_data.keys()\n",
    "print('Total Documents: ', len(all_doc_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この読み込んだjsonデータはこんな感じです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "', '.join(doc_data['715'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語のインデックスを作るために、全ての単語のリストを作ります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_vocab = []\n",
    "for doc_idx in all_doc_index:\n",
    "    all_vocab += doc_data[doc_idx]\n",
    "\n",
    "# 重複を消すためにsetしてlistにする\n",
    "all_vocab = list(set(all_vocab))\n",
    "vocab_num = len(all_vocab)\n",
    "print('Vocablary Number: ', vocab_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_doc_index_ar = np.array(list(all_doc_index))\n",
    "\n",
    "train_portion = 0.7\n",
    "train_num = int(len(all_doc_index_ar) * train_portion)\n",
    "\n",
    "np.random.shuffle(all_doc_index_ar)\n",
    "train_doc_index = all_doc_index_ar[:train_num]\n",
    "test_doc_index = all_doc_index_ar[train_num:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先にからっぽのスパース行列を定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A_train = sparse.lil_matrix((len(train_doc_index), len(all_vocab)),\n",
    "                            dtype=np.int)\n",
    "A_test = sparse.lil_matrix((len(test_doc_index), len(all_vocab)),\n",
    "                           dtype=np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ListからNumpyのArrayに直します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_vocab_ar = np.array(all_vocab)\n",
    "train_doc_index_ar = np.array(train_doc_index)\n",
    "test_doc_index_ar = np.array(test_doc_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "スパース行列に成分を入れていきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 学習用\n",
    "train_total_elements_num = 0\n",
    "for i in range(len(train_doc_index)):\n",
    "    doc_idx = train_doc_index[i]\n",
    "    row_data = Counter(doc_data[doc_idx])\n",
    "    \n",
    "    for word in row_data.keys():\n",
    "        word_idx = np.where(all_vocab_ar == word)[0][0]\n",
    "        A_train[i, word_idx] = row_data[word]\n",
    "        train_total_elements_num += 1\n",
    "print('Train total elements num :', train_total_elements_num)\n",
    "\n",
    "\n",
    "# テスト用\n",
    "test_total_elements_num = 0\n",
    "for i in range(len(test_doc_index)):\n",
    "    doc_idx = test_doc_index[i]\n",
    "    row_data = Counter(doc_data[doc_idx])\n",
    "    \n",
    "    for word in row_data.keys():\n",
    "        word_idx = np.where(all_vocab_ar == word)[0][0]\n",
    "        A_test[i, word_idx] = row_data[word]\n",
    "        test_total_elements_num += 1\n",
    "print('Test total elements num :', test_total_elements_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  CountVectorizerを用いてdoc_dataから簡単に疎行列を作ることもできます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=lambda a: a, analyzer=lambda a: a)\n",
    "vectorizer.fit(doc_data[idx] for idx in all_doc_index)\n",
    "A_train = vectorizer.transform(doc_data[idx] for idx in train_doc_index)\n",
    "A_test = vectorizer.transform(doc_data[idx] for idx in test_doc_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実際にLDAを適用してみよう (Scikit-learnを使った例）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = LatentDirichletAllocation(n_topics=20,\n",
    "                                   doc_topic_prior=0.001,\n",
    "                                   topic_word_prior=0.5,\n",
    "                                   max_iter=5,\n",
    "                                   learning_method='online',\n",
    "                                   learning_offset=50.,\n",
    "                                   random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1.fit(A_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずトピック x 単語を見てみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalize_components = model1.components_ / model1.components_.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/auto_examples/applications/\n",
    "# topics_extraction_with_nmf_lda.html　より\n",
    "n_top_words = 20\n",
    "for topic_idx, topic in enumerate(normalize_components):\n",
    "    print('Topic #%d:' % topic_idx)\n",
    "    print(' '.join([all_vocab_ar[i] for i in\n",
    "                    topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文書 x トピック行列側も見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_topic_data = model1.transform(A_train)\n",
    "doc_topic_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learnのLDAはどうやら正規化されていないため、正規化した上で、1つ目の文書がどのトピックから来ている単語が多いのかを見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalize_doc_topic_data = \\\n",
    " doc_topic_data / doc_topic_data.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for topic_idx, prob in enumerate(normalize_doc_topic_data[0]):\n",
    "    print('Topic #%d: probality: %f' % (topic_idx, prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loglikelihood = model1.score(A_test)\n",
    "ppl = model1.perplexity(A_test)\n",
    "print('対数尤度: ', loglikelihood)\n",
    "print('Perplexity: ', ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テストデータに当てはめてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_doc_topic_data = model1.transform(A_test)\n",
    "normalize_test_doc_topic_data = \\\n",
    " test_doc_topic_data / test_doc_topic_data.sum(axis=1, keepdims=True)\n",
    "for topic_idx, prob in enumerate(normalize_test_doc_topic_data[0]):\n",
    "    print('Topic #%d: probality: %f' % (topic_idx, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> LDAを適用してみよう (ldaパッケージを使った場合)</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2 = lda.LDA(n_topics=20, n_iter=1500, random_state=1, alpha=0.5, eta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model2.fit(A_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_word = model2.topic_word_\n",
    "n_top_words = 20\n",
    "for topic_idx, topic in enumerate(topic_word):\n",
    "    print('Topic #%d:' % topic_idx)\n",
    "    print(' '.join([all_vocab_ar[i] for i in\n",
    "                    topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回も精度として対数尤度を見てみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2.loglikelihood()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文書 x トピック行列を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_topic_data2 = model2.transform(A_train)\n",
    "for topic_idx, prob in enumerate(doc_topic_data2[0]):\n",
    "    print('Topic #%d: probality: %f' % (topic_idx, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>（参考）ディリクレ分布の挙動</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "K = 6\n",
    "sampled_probs = np.random.dirichlet([alpha for i in range(K)])\n",
    "for i, prob in enumerate(sampled_probs):\n",
    "    print('サイコロ %d面  確率: %.2f'%(i+1, prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
