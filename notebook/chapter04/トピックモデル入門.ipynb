{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "from __future__ import print_function\n",
    "\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import codecs\n",
    "\n",
    "import lda\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>データを読み込もう</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Documents:  1000\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.normpath('./dataset/document_word_data.json'), 'r') as f:\n",
    "    doc_data = json.load(f)\n",
    "\n",
    "all_doc_index = doc_data.keys()\n",
    "print('Total Documents: ', len(all_doc_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Windowsの方でutf-8のテキストファイルが開けない場合は、こちらを実行してみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Documents:  1000\n"
     ]
    }
   ],
   "source": [
    "# Windowsの方で上記のコードが失敗する場合\n",
    "with codecs.open(os.path.normpath('./dataset/document_word_data.json'), 'r', \"utf-8\") as f:\n",
    "    doc_data = json.load(f)\n",
    "\n",
    "all_doc_index = doc_data.keys()\n",
    "print('Total Documents: ', len(all_doc_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この読み込んだjsonデータはこんな感じです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ママ, 子供, 健康, づくり, 新た, ライフスタイル, 提案, ママ, マルシェ, 府, 府, 市, さまざま, 家族, ら, 日, もん, 商品, ほか, ハロ, 子供, 商品, プレゼント, 各日, 人, 会場, 木, ぬくもり, 木, 子供, づくり, 会, 木, 日, 午後, 時, 今年度, 森林, 林業, 木材, 大使, ミス, 日本, みどり, 帆, 南, さん, 府, 木材, 会, 湯川, 昌子, さん, ら, 女性, 人, 参加, スギ, ヒノキ, 木材, 放出, 健康, 効果, 木材, こと, 女性, 入場, 無料, 文化, 園, 入園, 料, 大人, 円, 円'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join(doc_data['715'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語のインデックスを作るために、全ての単語のリストを作ります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocablary Number:  8709\n"
     ]
    }
   ],
   "source": [
    "all_vocab = []\n",
    "for doc_idx in all_doc_index:\n",
    "    all_vocab += doc_data[doc_idx]\n",
    "\n",
    "# 重複を消すためにsetしてlistにする\n",
    "all_vocab = list(set(all_vocab))\n",
    "vocab_num = len(all_vocab)\n",
    "print('Vocablary Number: ', vocab_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_doc_index_ar = np.array(list(all_doc_index))\n",
    "\n",
    "train_portion = 0.7\n",
    "train_num = int(len(all_doc_index_ar) * train_portion)\n",
    "\n",
    "np.random.shuffle(all_doc_index_ar)\n",
    "train_doc_index = all_doc_index_ar[:train_num]\n",
    "test_doc_index = all_doc_index_ar[train_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_doc_index)\n",
    "len(test_doc_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先にからっぽのスパース行列を定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_train = sparse.lil_matrix((len(train_doc_index), len(all_vocab)),\n",
    "                            dtype=np.int)\n",
    "A_test = sparse.lil_matrix((len(test_doc_index), len(all_vocab)),\n",
    "                           dtype=np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ListからNumpyのArrayに直します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_vocab_ar = np.array(all_vocab)\n",
    "train_doc_index_ar = np.array(train_doc_index)\n",
    "test_doc_index_ar = np.array(test_doc_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['有利', 'ぼうこう', '幽か', ..., 'ココ', 'ララビュウ', 'ステッチ'], dtype='<U18')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_vocab_ar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([8706]),)\n"
     ]
    }
   ],
   "source": [
    "print(np.where(all_vocab_ar == 'ココ'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "スパース行列に成分を入れていきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train total elements num : 33591\n",
      "Test total elements num : 14453\n"
     ]
    }
   ],
   "source": [
    "# 学習用\n",
    "train_total_elements_num = 0\n",
    "for i in range(len(train_doc_index)):\n",
    "    doc_idx = train_doc_index[i]\n",
    "    row_data = Counter(doc_data[doc_idx])\n",
    "    \n",
    "    for word in row_data.keys():\n",
    "        word_idx = np.where(all_vocab_ar == word)[0][0]\n",
    "        print(word_idx)\n",
    "        A_train[i, word_idx] = row_data[word]\n",
    "        train_total_elements_num += 1\n",
    "print('Train total elements num :', train_total_elements_num)\n",
    "\n",
    "\n",
    "# テスト用\n",
    "test_total_elements_num = 0\n",
    "for i in range(len(test_doc_index)):\n",
    "    doc_idx = test_doc_index[i]\n",
    "    row_data = Counter(doc_data[doc_idx])\n",
    "    \n",
    "    for word in row_data.keys():\n",
    "        word_idx = np.where(all_vocab_ar == word)[0][0]\n",
    "        A_test[i, word_idx] = row_data[word]\n",
    "        test_total_elements_num += 1\n",
    "print('Test total elements num :', test_total_elements_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  CountVectorizerを用いてdoc_dataから簡単に疎行列を作ることもできます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=lambda a: a, analyzer=lambda a: a)\n",
    "vectorizer.fit(doc_data[idx] for idx in all_doc_index)\n",
    "A_train = vectorizer.transform(doc_data[idx] for idx in train_doc_index)\n",
    "A_test = vectorizer.transform(doc_data[idx] for idx in test_doc_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実際にLDAを適用してみよう (Scikit-learnを使った例）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = LatentDirichletAllocation(n_topics=20,\n",
    "                                   doc_topic_prior=0.001,\n",
    "                                   topic_word_prior=0.5,\n",
    "                                   max_iter=5,\n",
    "                                   learning_method='online',\n",
    "                                   learning_offset=50.,\n",
    "                                   random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/takahi-i/.pyenv/versions/3.6.0/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=0.001,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=50.0,\n",
       "             max_doc_update_iter=100, max_iter=5, mean_change_tol=0.001,\n",
       "             n_components=10, n_jobs=1, n_topics=20, perp_tol=0.1,\n",
       "             random_state=0, topic_word_prior=0.5, total_samples=1000000.0,\n",
       "             verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(A_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずトピック x 単語を見てみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_components = model1.components_ / model1.components_.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "メッキ とき カラオケアプリ 味噌 国営 スクエア 匂い ダメ グルテン 強 ウッチャン 志摩 心待ち 回 振り付け 本書 スポニチ さし パロアルト ガ\n",
      "\n",
      "Topic #1:\n",
      "スランプ 泥棒 ドデカイ 戦国 品 導師 ばら 浩 喜八 オタク 外国 マッサン ツヤ 枕 取りやめ ニオイ ワンポイント エッチ 入室 容\n",
      "\n",
      "Topic #2:\n",
      "エマナ 別人 切り出し 塁 ノイタミナ うちわ ぎゃるがん コ 実勢 協定 付け バルセロナキャンプ 女優 ランズ アダルト 他 気味 厚手 ウォッチ 手前\n",
      "\n",
      "Topic #3:\n",
      "ウィル 共和党 スタメン デスメタル 決済 ヘッドマウントディスプレイ 充分 ヴィブロス 大喜 来年度 合 キャビン オフショット 円 太もも セルフネイル ハロウィン シフト マンネリ 名声\n",
      "\n",
      "Topic #4:\n",
      "トゥットスポルト ビジョン レクサス 奥さん 戸塚 ウォッシュレット 交差点 出場 シンク ベネズエラ たいよう ダンス 凱旋 ロング ベイベ 将来 平塚 改憲 リッチ チカラ\n",
      "\n",
      "Topic #5:\n",
      "復帰 次元 島崎 アミュプラザ おじ 少年 日常 ブラウン にいさん 兆 尻 まちなか 国広 ガガ ドコモ 完全 向け ニキビ クロス 栗子\n",
      "\n",
      "Topic #6:\n",
      "京子 モロヘイヤ 点数 タロウ 台東 お悔やみ 栗山 ボンボン プレゼンス ダロワイヨ むや 担保 ヒユウザン ビフィズス ドリブル 武司 セ チャント クリントン てん\n",
      "\n",
      "Topic #7:\n",
      "樋口 就学 ランキング さ タブ いっしょ 本体 滑 まんま 毛 こまめ メカニズム コンビニ プラティニ 吉川 人 スタンプ ナゾ 温暖 ブロフェス\n",
      "\n",
      "Topic #8:\n",
      "検察 江北 ラメ 住処 コシ 沿岸 段数 ダウ ヒト 伝染 オブ 滅多 タオル ラッチ 将希 応援 サウンド 可 ブルックリン 決意\n",
      "\n",
      "Topic #9:\n",
      "前倒し 混入 検出 楽 可塑 ウォッシュレット 専業 時代 合体 凄惨 東屋 低俗 低め 月刊 オレンジ だらけ 同情 ジワッ パッティング 刃\n",
      "\n",
      "Topic #10:\n",
      "スジョン ツイン 意気投合 イブニングキャンプ キュン 技法 オビ 招き ヘクセイタス スポンジ ヒョウ エチケット 同日 振幅 点検 マイルド だし ハイレベル 展望 庭\n",
      "\n",
      "Topic #11:\n",
      "パレット ば 俣宏 シエル 大倉山 余り 奥村 彅健 寒梅 作成 振動 宮 リアクション ミラノ 打ち合わせ ブライアン 唐津 マニュアル アウト 加入\n",
      "\n",
      "Topic #12:\n",
      "セリ ブルゾン 寿命 村内 シイタケ メイメイ 商業 ジェイルブレイク 所定 アボカド プミポン 影 リナ フィット フェチ 報告 感性 ガッツ 休み 煌\n",
      "\n",
      "Topic #13:\n",
      "反復 初年度 古墳 その後 厚み 沙 尖塔 ロイヤリティ テクニカル 最初 廿日市 準決勝 エリック 潜水 ピン 晃司 ハンド トヨタ かなり タイズ\n",
      "\n",
      "Topic #14:\n",
      "コダック シット 損失 キッズプログラム テラコッタ 愛 バル 旅先 偉大 来夏 宇治 エフエフ 偉業 パシフ トッピング 火力 ひぼう 壽 ポスト 暖\n",
      "\n",
      "Topic #15:\n",
      "愛情 文則 恩人 ジョナサン ワッフル キッド ウィメンズ アニキ カップ 急増 怠慢 冠 ひかり アイコン スロット ナイフ 値段 岡崎 ナカ 映像\n",
      "\n",
      "Topic #16:\n",
      "チェンジ メゾンフレグランス アイツ 原作 パイロット 伝染 カイロ 弓 ガンヴォルト 悪 イエメン 伊 エスニック 大はしゃぎ プレゼン ジュロン フォン 慶大 バッチャ ラッシュ\n",
      "\n",
      "Topic #17:\n",
      "四条 ピグ 津川 依存 区域 椿 ポ 昨日 塩尻 更新 やまもと サリン アニメイト 毒殺 暁子 宿命 格差 流浪 気掛かり ワタ\n",
      "\n",
      "Topic #18:\n",
      "好守 シソ 応急 坂東 ミッション 可決 入れ替え 佳正 河川 初代 うっかり 内側 すう勢 ぇ プロフ サンドウィッチマン こうえん 充彦 ぶつ フィッシング\n",
      "\n",
      "Topic #19:\n",
      "サツマイモ 勢多 クリアランスランプ エニックス 深沢 布施 みろく エキストラ アレッポ 千尋 エッフェル塔 支所 出だし エンパイア 新株 大口 代打 威嚇 参入 ドキドキ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# http://scikit-learn.org/stable/auto_examples/applications/\n",
    "# topics_extraction_with_nmf_lda.html　より\n",
    "n_top_words = 20\n",
    "for topic_idx, topic in enumerate(normalize_components):\n",
    "    print('Topic #%d:' % topic_idx)\n",
    "    print(' '.join([all_vocab_ar[i] for i in\n",
    "                    topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文書 x トピック行列側も見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.12675457e-05, 2.12675457e-05, 2.12675457e-05, ...,\n",
       "        6.17106391e-01, 2.12675457e-05, 2.12675457e-05],\n",
       "       [6.99202909e-06, 6.99202909e-06, 6.99202909e-06, ...,\n",
       "        9.99867151e-01, 6.99202909e-06, 6.99202909e-06],\n",
       "       [1.66611130e-05, 1.66611130e-05, 1.66611130e-05, ...,\n",
       "        8.01512246e-01, 1.66611130e-05, 1.66611130e-05],\n",
       "       ...,\n",
       "       [5.25762355e-05, 5.25762355e-05, 5.25762355e-05, ...,\n",
       "        9.99001052e-01, 5.25762355e-05, 5.25762355e-05],\n",
       "       [4.54132607e-05, 4.54132607e-05, 4.54132607e-05, ...,\n",
       "        4.54132607e-05, 4.54132607e-05, 4.54132607e-05],\n",
       "       [8.40194925e-06, 8.40194925e-06, 8.40194925e-06, ...,\n",
       "        9.99840363e-01, 8.40194925e-06, 8.40194925e-06]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic_data = model1.transform(A_train)\n",
    "doc_topic_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learnのLDAはどうやら正規化されていないため、正規化した上で、1つ目の文書がどのトピックから来ている単語が多いのかを見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_doc_topic_data = \\\n",
    " doc_topic_data / doc_topic_data.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: probality: 0.000021\n",
      "Topic #1: probality: 0.000021\n",
      "Topic #2: probality: 0.000021\n",
      "Topic #3: probality: 0.000021\n",
      "Topic #4: probality: 0.000021\n",
      "Topic #5: probality: 0.000021\n",
      "Topic #6: probality: 0.000021\n",
      "Topic #7: probality: 0.000021\n",
      "Topic #8: probality: 0.382511\n",
      "Topic #9: probality: 0.000021\n",
      "Topic #10: probality: 0.000021\n",
      "Topic #11: probality: 0.000021\n",
      "Topic #12: probality: 0.000021\n",
      "Topic #13: probality: 0.000021\n",
      "Topic #14: probality: 0.000021\n",
      "Topic #15: probality: 0.000021\n",
      "Topic #16: probality: 0.000021\n",
      "Topic #17: probality: 0.617106\n",
      "Topic #18: probality: 0.000021\n",
      "Topic #19: probality: 0.000021\n"
     ]
    }
   ],
   "source": [
    "for topic_idx, prob in enumerate(normalize_doc_topic_data[0]):\n",
    "    print('Topic #%d: probality: %f' % (topic_idx, prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "対数尤度:  -215572.08399223955\n",
      "Perplexity:  4795.394248686762\n"
     ]
    }
   ],
   "source": [
    "loglikelihood = model1.score(A_test)\n",
    "ppl = model1.perplexity(A_test)\n",
    "print('対数尤度: ', loglikelihood)\n",
    "print('Perplexity: ', ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テストデータに当てはめてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: probality: 0.000014\n",
      "Topic #1: probality: 0.000014\n",
      "Topic #2: probality: 0.000014\n",
      "Topic #3: probality: 0.582879\n",
      "Topic #4: probality: 0.000014\n",
      "Topic #5: probality: 0.000014\n",
      "Topic #6: probality: 0.000014\n",
      "Topic #7: probality: 0.000014\n",
      "Topic #8: probality: 0.000014\n",
      "Topic #9: probality: 0.000014\n",
      "Topic #10: probality: 0.000014\n",
      "Topic #11: probality: 0.000014\n",
      "Topic #12: probality: 0.000014\n",
      "Topic #13: probality: 0.000014\n",
      "Topic #14: probality: 0.000014\n",
      "Topic #15: probality: 0.000014\n",
      "Topic #16: probality: 0.000014\n",
      "Topic #17: probality: 0.416864\n",
      "Topic #18: probality: 0.000014\n",
      "Topic #19: probality: 0.000014\n"
     ]
    }
   ],
   "source": [
    "test_doc_topic_data = model1.transform(A_test)\n",
    "normalize_test_doc_topic_data = \\\n",
    " test_doc_topic_data / test_doc_topic_data.sum(axis=1, keepdims=True)\n",
    "for topic_idx, prob in enumerate(normalize_test_doc_topic_data[0]):\n",
    "    print('Topic #%d: probality: %f' % (topic_idx, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> LDAを適用してみよう (ldaパッケージを使った場合)</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = lda.LDA(n_topics=20, n_iter=1500, random_state=1, alpha=0.5, eta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 700\n",
      "INFO:lda:vocab_size: 8709\n",
      "INFO:lda:n_words: 60449\n",
      "INFO:lda:n_topics: 20\n",
      "INFO:lda:n_iter: 1500\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "/Users/takahi-i/.pyenv/versions/3.6.0/lib/python3.6/site-packages/lda/utils.py:55: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if sparse and not np.issubdtype(doc_word.dtype, int):\n",
      "INFO:lda:<0> log likelihood: -690107\n",
      "INFO:lda:<10> log likelihood: -537616\n",
      "INFO:lda:<20> log likelihood: -508857\n",
      "INFO:lda:<30> log likelihood: -504624\n",
      "INFO:lda:<40> log likelihood: -502809\n",
      "INFO:lda:<50> log likelihood: -502036\n",
      "INFO:lda:<60> log likelihood: -501267\n",
      "INFO:lda:<70> log likelihood: -501386\n",
      "INFO:lda:<80> log likelihood: -501062\n",
      "INFO:lda:<90> log likelihood: -500661\n",
      "INFO:lda:<100> log likelihood: -500856\n",
      "INFO:lda:<110> log likelihood: -500983\n",
      "INFO:lda:<120> log likelihood: -500999\n",
      "INFO:lda:<130> log likelihood: -500818\n",
      "INFO:lda:<140> log likelihood: -500299\n",
      "INFO:lda:<150> log likelihood: -500792\n",
      "INFO:lda:<160> log likelihood: -500479\n",
      "INFO:lda:<170> log likelihood: -500357\n",
      "INFO:lda:<180> log likelihood: -500668\n",
      "INFO:lda:<190> log likelihood: -500813\n",
      "INFO:lda:<200> log likelihood: -500831\n",
      "INFO:lda:<210> log likelihood: -500627\n",
      "INFO:lda:<220> log likelihood: -500741\n",
      "INFO:lda:<230> log likelihood: -500930\n",
      "INFO:lda:<240> log likelihood: -501147\n",
      "INFO:lda:<250> log likelihood: -500832\n",
      "INFO:lda:<260> log likelihood: -500820\n",
      "INFO:lda:<270> log likelihood: -500813\n",
      "INFO:lda:<280> log likelihood: -500210\n",
      "INFO:lda:<290> log likelihood: -500166\n",
      "INFO:lda:<300> log likelihood: -500530\n",
      "INFO:lda:<310> log likelihood: -500933\n",
      "INFO:lda:<320> log likelihood: -501216\n",
      "INFO:lda:<330> log likelihood: -500969\n",
      "INFO:lda:<340> log likelihood: -500945\n",
      "INFO:lda:<350> log likelihood: -500546\n",
      "INFO:lda:<360> log likelihood: -500705\n",
      "INFO:lda:<370> log likelihood: -500643\n",
      "INFO:lda:<380> log likelihood: -500937\n",
      "INFO:lda:<390> log likelihood: -501124\n",
      "INFO:lda:<400> log likelihood: -499945\n",
      "INFO:lda:<410> log likelihood: -501280\n",
      "INFO:lda:<420> log likelihood: -501219\n",
      "INFO:lda:<430> log likelihood: -500616\n",
      "INFO:lda:<440> log likelihood: -500940\n",
      "INFO:lda:<450> log likelihood: -500303\n",
      "INFO:lda:<460> log likelihood: -500731\n",
      "INFO:lda:<470> log likelihood: -500569\n",
      "INFO:lda:<480> log likelihood: -500394\n",
      "INFO:lda:<490> log likelihood: -500783\n",
      "INFO:lda:<500> log likelihood: -499819\n",
      "INFO:lda:<510> log likelihood: -501086\n",
      "INFO:lda:<520> log likelihood: -501139\n",
      "INFO:lda:<530> log likelihood: -500126\n",
      "INFO:lda:<540> log likelihood: -500090\n",
      "INFO:lda:<550> log likelihood: -500577\n",
      "INFO:lda:<560> log likelihood: -500106\n",
      "INFO:lda:<570> log likelihood: -500122\n",
      "INFO:lda:<580> log likelihood: -500787\n",
      "INFO:lda:<590> log likelihood: -500808\n",
      "INFO:lda:<600> log likelihood: -501454\n",
      "INFO:lda:<610> log likelihood: -500400\n",
      "INFO:lda:<620> log likelihood: -500897\n",
      "INFO:lda:<630> log likelihood: -500015\n",
      "INFO:lda:<640> log likelihood: -500693\n",
      "INFO:lda:<650> log likelihood: -500649\n",
      "INFO:lda:<660> log likelihood: -500707\n",
      "INFO:lda:<670> log likelihood: -500952\n",
      "INFO:lda:<680> log likelihood: -501064\n",
      "INFO:lda:<690> log likelihood: -500193\n",
      "INFO:lda:<700> log likelihood: -500092\n",
      "INFO:lda:<710> log likelihood: -500917\n",
      "INFO:lda:<720> log likelihood: -500472\n",
      "INFO:lda:<730> log likelihood: -499962\n",
      "INFO:lda:<740> log likelihood: -500814\n",
      "INFO:lda:<750> log likelihood: -500826\n",
      "INFO:lda:<760> log likelihood: -500021\n",
      "INFO:lda:<770> log likelihood: -500746\n",
      "INFO:lda:<780> log likelihood: -499813\n",
      "INFO:lda:<790> log likelihood: -500686\n",
      "INFO:lda:<800> log likelihood: -500653\n",
      "INFO:lda:<810> log likelihood: -500640\n",
      "INFO:lda:<820> log likelihood: -500468\n",
      "INFO:lda:<830> log likelihood: -500997\n",
      "INFO:lda:<840> log likelihood: -500728\n",
      "INFO:lda:<850> log likelihood: -500521\n",
      "INFO:lda:<860> log likelihood: -500493\n",
      "INFO:lda:<870> log likelihood: -500448\n",
      "INFO:lda:<880> log likelihood: -500644\n",
      "INFO:lda:<890> log likelihood: -500000\n",
      "INFO:lda:<900> log likelihood: -500649\n",
      "INFO:lda:<910> log likelihood: -500880\n",
      "INFO:lda:<920> log likelihood: -500765\n",
      "INFO:lda:<930> log likelihood: -500602\n",
      "INFO:lda:<940> log likelihood: -500717\n",
      "INFO:lda:<950> log likelihood: -499857\n",
      "INFO:lda:<960> log likelihood: -499963\n",
      "INFO:lda:<970> log likelihood: -500927\n",
      "INFO:lda:<980> log likelihood: -500583\n",
      "INFO:lda:<990> log likelihood: -500487\n",
      "INFO:lda:<1000> log likelihood: -500282\n",
      "INFO:lda:<1010> log likelihood: -501026\n",
      "INFO:lda:<1020> log likelihood: -500183\n",
      "INFO:lda:<1030> log likelihood: -500720\n",
      "INFO:lda:<1040> log likelihood: -500474\n",
      "INFO:lda:<1050> log likelihood: -500456\n",
      "INFO:lda:<1060> log likelihood: -500569\n",
      "INFO:lda:<1070> log likelihood: -500000\n",
      "INFO:lda:<1080> log likelihood: -500113\n",
      "INFO:lda:<1090> log likelihood: -500681\n",
      "INFO:lda:<1100> log likelihood: -500908\n",
      "INFO:lda:<1110> log likelihood: -500451\n",
      "INFO:lda:<1120> log likelihood: -500764\n",
      "INFO:lda:<1130> log likelihood: -500808\n",
      "INFO:lda:<1140> log likelihood: -500356\n",
      "INFO:lda:<1150> log likelihood: -500603\n",
      "INFO:lda:<1160> log likelihood: -500441\n",
      "INFO:lda:<1170> log likelihood: -499771\n",
      "INFO:lda:<1180> log likelihood: -501162\n",
      "INFO:lda:<1190> log likelihood: -499745\n",
      "INFO:lda:<1200> log likelihood: -500100\n",
      "INFO:lda:<1210> log likelihood: -500433\n",
      "INFO:lda:<1220> log likelihood: -499956\n",
      "INFO:lda:<1230> log likelihood: -501092\n",
      "INFO:lda:<1240> log likelihood: -500919\n",
      "INFO:lda:<1250> log likelihood: -500193\n",
      "INFO:lda:<1260> log likelihood: -501045\n",
      "INFO:lda:<1270> log likelihood: -500317\n",
      "INFO:lda:<1280> log likelihood: -500357\n",
      "INFO:lda:<1290> log likelihood: -499617\n",
      "INFO:lda:<1300> log likelihood: -500809\n",
      "INFO:lda:<1310> log likelihood: -500501\n",
      "INFO:lda:<1320> log likelihood: -500393\n",
      "INFO:lda:<1330> log likelihood: -501118\n",
      "INFO:lda:<1340> log likelihood: -499988\n",
      "INFO:lda:<1350> log likelihood: -501056\n",
      "INFO:lda:<1360> log likelihood: -500700\n",
      "INFO:lda:<1370> log likelihood: -500370\n",
      "INFO:lda:<1380> log likelihood: -500879\n",
      "INFO:lda:<1390> log likelihood: -499709\n",
      "INFO:lda:<1400> log likelihood: -501235\n",
      "INFO:lda:<1410> log likelihood: -500829\n",
      "INFO:lda:<1420> log likelihood: -500369\n",
      "INFO:lda:<1430> log likelihood: -501198\n",
      "INFO:lda:<1440> log likelihood: -500635\n",
      "INFO:lda:<1450> log likelihood: -500795\n",
      "INFO:lda:<1460> log likelihood: -500329\n",
      "INFO:lda:<1470> log likelihood: -500274\n",
      "INFO:lda:<1480> log likelihood: -500650\n",
      "INFO:lda:<1490> log likelihood: -500915\n",
      "INFO:lda:<1499> log likelihood: -500732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lda.lda.LDA at 0x115b93588>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(A_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "格差 易 リンゴ 広瀬 お呼び 充当 域外 コラ 公明党 ブランディング 悲しみ ひよこ 歌 キッド 向き 意地 極寒 キャリア ラテックス 大手\n",
      "\n",
      "Topic #1:\n",
      "ワイヤ 召使い 圧勝 挽回 同右 岩波書店 やまもと 更新 ビル 大洲 ハンドル オネエ 同列 夏バテ シンサドン 元帥 創価大 夫 ふん ジャングル\n",
      "\n",
      "Topic #2:\n",
      "修士 博愛 ズブ 本来 指人形 模型 ボタン 激変 敏文 少子化 ぁぁあ 因子 ネットワ 受信 ロイヤルティ 晴れ ミニッツ 市原 ブリキ ひと安心\n",
      "\n",
      "Topic #3:\n",
      "悲劇 ワタ 昨日 おもてなし トモダチ 媛 来年度 椿 昨年 依存 択 橋本 気掛かり 内海 春愁 びねり 四条 本拠 やまもと 日本海\n",
      "\n",
      "Topic #4:\n",
      "ノイタミナ スランプ ランズ アダルト 泥棒 ドデカイ エデル 喜八 点字 シンバ エルメス 戦国 指令 典 のぶ 常時 健 ヘッドアップディスプレイ 外国 いい加減\n",
      "\n",
      "Topic #5:\n",
      "フルサイズ リッチ げた イニング 勇士 モコモコ ビジョン アレルバリア 先 南北 入念 とうもろこし ドリル デスク トゥットスポルト ポチ 将来 志村 アスパラガス ベネズエラ\n",
      "\n",
      "Topic #6:\n",
      "依存 区域 四条 津川 トヨ ポ アニメイト 流浪 やまもと ピグ ホイッスル 毒殺 喫煙 更新 大人 有名 おもてなし 柔らか ウルフ 昨日\n",
      "\n",
      "Topic #7:\n",
      "指数 別人 エマナ 切り出し 援助 実勢 ラジアルファン 挑 バルセロナキャンプ サイト 日活 ピンチ 態度 松吉 アイテム 朴 ハイカラ 厚手 役柄 気味\n",
      "\n",
      "Topic #8:\n",
      "宿命 入学 坂本 悲劇 ふみ クッション 樋口 ホアンキエム 昨日 キュンセリフ おしゃぶり ランキング シュシュ イングランド こまめ 勝人 ナイン 土日 マリリンジョイ テスト\n",
      "\n",
      "Topic #9:\n",
      "ピグ 昨日 ワタ 暁子 ウィル デスメタル 水平 ラベル いっしょ 制御 タブ 太もも 合 エルメス オフホワイト 大助 エルス アリストトリスト ルナ サンリオ\n",
      "\n",
      "Topic #10:\n",
      "宿命 塩尻 作家 今年度 タイヤ シリアス 流浪 あすか 比呂志 イタズラ ラベアン 根っこ 太平洋 ケバ 宮津 サガン 海流 メタル サザンセト 叫び\n",
      "\n",
      "Topic #11:\n",
      "ブラッシュ 悲劇 指数 周 昨日 チェストプレス アクティビティ 海 正念場 明治 カニサレス アリストテレス アプリ サロモン サラゴナ 刀 ソックス すう勢 ミッション すね\n",
      "\n",
      "Topic #12:\n",
      "ピグ 例文 バスケ 坪 差 明暗 ヴェンゲル ウィズ ヒズボラ コク 横 噂 名優 ゾンビ 教材 更新 まつげ ケア かたぎ ヨハネ\n",
      "\n",
      "Topic #13:\n",
      "光沢 ぎゃるがん 国 カルビ いさ 木曜 ライトアップ 代役 清潔 インフレ 女優 測定 チュニジア ハガキ 手前 帽 メセナ おかず 正当 あなた\n",
      "\n",
      "Topic #14:\n",
      "シャインビル ディレク ジュ 吹き抜け 善悪 巨災 りか 受け入れ 大局 ブラ 東広島 海人 柿原 区役所 半年 日数 オチンチン フルラインキット 先日 剣先\n",
      "\n",
      "Topic #15:\n",
      "コントラスト 氏家 学年 復刻 むら 悲劇 以来 ラフ 応え くじ スペダン めまい トレッサ 愛情 光央 横堀 スキン 棘 シンクライアント 温水\n",
      "\n",
      "Topic #16:\n",
      "来年度 円 共和党 決済 充分 アリストテレス オフショット 多人数 アウディ びねり さゆり やり手 名声 ステッチ キヌア ヴィブロス マンネリ マスカラ ハロウィン 廃棄\n",
      "\n",
      "Topic #17:\n",
      "しめ 光 備え ショック コンベンション ネットワ エジル 拓 切り 岳彦 ロマン ロペピクニック 注射 出席 拡大 動機 ハイカラ タンパク 前売り 明暗\n",
      "\n",
      "Topic #18:\n",
      "ゆう子 のこぎり ドリンク チェンジ コミュニティ 名匠 喉 抵抗 内側 小園 ニドラン トリ プロレス シンバ ガンヴォルト コミック 坂東 伝染 横堀 小島\n",
      "\n",
      "Topic #19:\n",
      "宿命 塩尻 ポチ 前年 有名 出国 博愛 姫島 守 各位 東灘 かぼちゃ 強引 スタメン 容器 ブリンケン オニオン 噂 クッキリ 密\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic_word = model2.topic_word_\n",
    "n_top_words = 20\n",
    "for topic_idx, topic in enumerate(topic_word):\n",
    "    print('Topic #%d:' % topic_idx)\n",
    "    print(' '.join([all_vocab_ar[i] for i in\n",
    "                    topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回も精度として対数尤度を見てみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-500732.333847963"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.loglikelihood()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文書 x トピック行列を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero row in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "/Users/takahi-i/.pyenv/versions/3.6.0/lib/python3.6/site-packages/lda/utils.py:55: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if sparse and not np.issubdtype(doc_word.dtype, int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: probality: 0.033475\n",
      "Topic #1: probality: 0.011446\n",
      "Topic #2: probality: 0.005824\n",
      "Topic #3: probality: 0.032966\n",
      "Topic #4: probality: 0.005500\n",
      "Topic #5: probality: 0.005773\n",
      "Topic #6: probality: 0.021254\n",
      "Topic #7: probality: 0.043399\n",
      "Topic #8: probality: 0.009719\n",
      "Topic #9: probality: 0.019212\n",
      "Topic #10: probality: 0.091827\n",
      "Topic #11: probality: 0.648025\n",
      "Topic #12: probality: 0.010089\n",
      "Topic #13: probality: 0.006127\n",
      "Topic #14: probality: 0.005800\n",
      "Topic #15: probality: 0.006053\n",
      "Topic #16: probality: 0.017921\n",
      "Topic #17: probality: 0.007934\n",
      "Topic #18: probality: 0.013296\n",
      "Topic #19: probality: 0.004362\n"
     ]
    }
   ],
   "source": [
    "doc_topic_data2 = model2.transform(A_train)\n",
    "for topic_idx, prob in enumerate(doc_topic_data2[0]):\n",
    "    print('Topic #%d: probality: %f' % (topic_idx, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>（参考）ディリクレ分布の挙動</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "サイコロ 1面  確率: 0.40\n",
      "サイコロ 2面  確率: 0.00\n",
      "サイコロ 3面  確率: 0.60\n",
      "サイコロ 4面  確率: 0.00\n",
      "サイコロ 5面  確率: 0.00\n",
      "サイコロ 6面  確率: 0.00\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "K = 6\n",
    "sampled_probs = np.random.dirichlet([alpha for i in range(K)])\n",
    "for i, prob in enumerate(sampled_probs):\n",
    "    print('サイコロ %d面  確率: %.2f'%(i+1, prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
